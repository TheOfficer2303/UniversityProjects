{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cphH-e6Mq3ji"
   },
   "source": [
    "# 7 Recurrent neural networks\n",
    "In this exercise we will try a simple experiment with a recurrent neural network. One of the well-known recurrent neural network models is the so called Long short-term memory (LSTM) network. More information on LSTM can be found in the text [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "## 7.1 The MNIST dataset revisited (1)\n",
    "In one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a recurrent neural network to the problem of digits classification. To keep it simple, we will use a simple LSTM network that will be fed with one row of the image at a time. With each new row, it will update its states and give its prediction. What we are interested in is its prediction after the last row i.e. after it has the full information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h69cwQIWq3jm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 96.29%\n",
      "Epoch [2/10], Test Accuracy: 96.41%\n",
      "Epoch [3/10], Test Accuracy: 97.80%\n",
      "Epoch [4/10], Test Accuracy: 98.26%\n",
      "Epoch [5/10], Test Accuracy: 98.29%\n",
      "Epoch [6/10], Test Accuracy: 98.45%\n",
      "Epoch [7/10], Test Accuracy: 98.46%\n",
      "Epoch [8/10], Test Accuracy: 98.31%\n",
      "Epoch [9/10], Test Accuracy: 98.63%\n",
      "Epoch [10/10], Test Accuracy: 98.62%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.62"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train(rows):\n",
    "    # Constants\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10\n",
    "    batch_size = 100\n",
    "    #we will feed a row at a time to the LSTM and there are 28 rows per image\n",
    "    timesteps = 28\n",
    "    #each row has 28 columns whose values are simultaneously passed to LSTM\n",
    "    n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "    #the number of hidden states in the LSTM\n",
    "    n_hidden = 128\n",
    "    n_classes = 10\n",
    "\n",
    "    # Data transformation\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the LSTM model\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            out, _ = self.lstm(x, (h0, c0))\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "    # Initialize the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTMModel(n_input, n_hidden, 1, n_classes).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.view(-1, timesteps, n_input)[:, :rows, :].to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Test the model\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in test_loader:\n",
    "                    images = images.view(-1, timesteps, n_input).to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "train(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 10.57%\n",
      "Epoch [2/10], Test Accuracy: 12.81%\n",
      "Epoch [3/10], Test Accuracy: 11.22%\n",
      "Epoch [4/10], Test Accuracy: 13.29%\n",
      "Epoch [5/10], Test Accuracy: 10.92%\n",
      "Epoch [6/10], Test Accuracy: 11.71%\n",
      "Epoch [7/10], Test Accuracy: 11.95%\n",
      "Epoch [8/10], Test Accuracy: 12.42%\n",
      "Epoch [9/10], Test Accuracy: 12.27%\n",
      "Epoch [10/10], Test Accuracy: 11.54%\n",
      "Epoch [1/10], Test Accuracy: 7.98%\n",
      "Epoch [2/10], Test Accuracy: 9.71%\n",
      "Epoch [3/10], Test Accuracy: 9.80%\n",
      "Epoch [4/10], Test Accuracy: 9.62%\n",
      "Epoch [5/10], Test Accuracy: 10.45%\n",
      "Epoch [6/10], Test Accuracy: 9.19%\n",
      "Epoch [7/10], Test Accuracy: 9.40%\n",
      "Epoch [8/10], Test Accuracy: 12.34%\n",
      "Epoch [9/10], Test Accuracy: 11.02%\n",
      "Epoch [10/10], Test Accuracy: 13.38%\n",
      "Epoch [1/10], Test Accuracy: 19.28%\n",
      "Epoch [2/10], Test Accuracy: 19.06%\n",
      "Epoch [3/10], Test Accuracy: 20.64%\n",
      "Epoch [4/10], Test Accuracy: 25.54%\n",
      "Epoch [5/10], Test Accuracy: 21.32%\n",
      "Epoch [6/10], Test Accuracy: 27.87%\n",
      "Epoch [7/10], Test Accuracy: 26.25%\n",
      "Epoch [8/10], Test Accuracy: 23.54%\n",
      "Epoch [9/10], Test Accuracy: 22.38%\n",
      "Epoch [10/10], Test Accuracy: 22.95%\n",
      "Epoch [1/10], Test Accuracy: 20.12%\n",
      "Epoch [2/10], Test Accuracy: 20.58%\n",
      "Epoch [3/10], Test Accuracy: 22.35%\n",
      "Epoch [4/10], Test Accuracy: 21.06%\n",
      "Epoch [5/10], Test Accuracy: 24.85%\n",
      "Epoch [6/10], Test Accuracy: 19.47%\n",
      "Epoch [7/10], Test Accuracy: 19.49%\n",
      "Epoch [8/10], Test Accuracy: 20.61%\n",
      "Epoch [9/10], Test Accuracy: 22.22%\n",
      "Epoch [10/10], Test Accuracy: 19.38%\n",
      "Epoch [1/10], Test Accuracy: 69.98%\n",
      "Epoch [2/10], Test Accuracy: 78.19%\n",
      "Epoch [3/10], Test Accuracy: 74.22%\n",
      "Epoch [4/10], Test Accuracy: 78.40%\n",
      "Epoch [5/10], Test Accuracy: 75.76%\n",
      "Epoch [6/10], Test Accuracy: 81.01%\n",
      "Epoch [7/10], Test Accuracy: 82.48%\n",
      "Epoch [8/10], Test Accuracy: 84.40%\n",
      "Epoch [9/10], Test Accuracy: 82.15%\n",
      "Epoch [10/10], Test Accuracy: 86.84%\n",
      "Epoch [1/10], Test Accuracy: 95.54%\n",
      "Epoch [2/10], Test Accuracy: 97.22%\n",
      "Epoch [3/10], Test Accuracy: 98.14%\n",
      "Epoch [4/10], Test Accuracy: 98.41%\n",
      "Epoch [5/10], Test Accuracy: 98.33%\n",
      "Epoch [6/10], Test Accuracy: 98.76%\n",
      "Epoch [7/10], Test Accuracy: 98.66%\n",
      "Epoch [8/10], Test Accuracy: 98.63%\n",
      "Epoch [9/10], Test Accuracy: 98.69%\n",
      "Epoch [10/10], Test Accuracy: 98.77%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "accuracy_values = []\n",
    "rows = [3, 10, 15, 18, 23, 28]\n",
    "\n",
    "for row in rows:\n",
    "    accuracy = train(row)\n",
    "    accuracy_values.append(accuracy)\n",
    "\n",
    "plt.plot(rows, accuracy_values)\n",
    "plt.xlabel(\"Number of rows\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "iRbpYMdrq3jn"
   },
   "source": [
    "**Tasks**\n",
    "\n",
    "1. Study and run the code above.\n",
    "2. Draw a plot that shows the relation between the number of rows given to the network and its final accuracy on the test set.\n",
    "3. What happens if we use gradient descent instead of Adam?\n",
    "\n",
    "    Accuracy would drop significantly. Code is shown below, I accidentally cleared output of the cell and I do not have enough time to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train(rows):\n",
    "    # Constants\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10\n",
    "    batch_size = 100\n",
    "    #we will feed a row at a time to the LSTM and there are 28 rows per image\n",
    "    timesteps = 28\n",
    "    #each row has 28 columns whose values are simultaneously passed to LSTM\n",
    "    n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "    #the number of hidden states in the LSTM\n",
    "    n_hidden = 128\n",
    "    n_classes = 10\n",
    "\n",
    "    # Data transformation\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the LSTM model\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            out, _ = self.lstm(x, (h0, c0))\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "    # Initialize the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTMModel(n_input, n_hidden, 1, n_classes).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.view(-1, timesteps, n_input)[:, :rows, :].to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Test the model\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in test_loader:\n",
    "                    images = images.view(-1, timesteps, n_input).to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "train(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNpp9Ppweg3Q"
   },
   "source": [
    "# 8 Convolutional neural networks\n",
    "\n",
    "\n",
    "The goal of this exercise is to learn the basic stuff about [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN or ConvNet). In the previous exercises, the building blocks mostly included simple operations that had some kind of activations functions and each layer was usually fully connected to the previous one. CNNs take into account the spatial nature of the input data, e.g. an image, and they process it by applying one or more  [kernels](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29). In the case of images, this processing i.e. convolving is also known as filtering. The results of processing the input with a single kernel will be a single channel, but usually a convolutional layer involves more kernels producing more channels. These channels are often called **feature maps** because each kernel is specialized for extraction of a certain kind of features from the input. These feature maps are then combined into a single tensor that can be viewed as an image with multiple channels that can be then passed to further convolutional layers.\n",
    "\n",
    "For example, if the input consists of a grayscale image i.e. an image with only one channel and a $5\\times 5$ kernel is applied, the result is a single feature map. The borders of the input image are usually padded with zeros to ensure that the resulting feature maps has the same number of rows and columns as the input image.\n",
    "\n",
    "If the input consists of a color image i.e. an image with three channels and a $5\\times 5$ kernel is applied, what will actually be applied is an $5\\times 5\\times 3$ kernel that will simultaneously process all three channels and the result will again be a single feature map. However, if e.g. 16 several kernels are applied, then the result will be 16 feature maps. Should they be passed to another convolutional layer, **each** of its kernels would simultaneously process **all** feature maps so their sizes would be e.g. $3\\times 3\\times 16$ or $5\\times 5\\times 16$ where 16 is used to reach all feature maps simultaneously.\n",
    "\n",
    "The convolution is usually followed by applying an element-wise non-linear operation to each of the values in the feature maps. Finally, what often follows is the summarization i.e. pooling of the information in the feature maps to reduce the spatial dimensions and keep only the most important information. A common approach used here is the so called max pooling. It is a non-linear downsampling where the input is divided into a set of non-overlapping rectangles and for each of them only the maximum value inside of it is kept.\n",
    "\n",
    "![Model of a neuron](data:image/png;base64,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)\n",
    "<center>Figure 1. Max pooling with $2\\times 2$ rectangles (taken from [Wikipedia](https://en.wikipedia.org/wiki/File:Max_pooling.png)).</center>\n",
    "\n",
    "What usually follows after several convolutional layers is putting the values of all feature maps into a single vector, which is then passed further to fully connected or other kinds of layers.\n",
    "\n",
    "The number of parameters in the convolutional depends on the number of feature maps and the sizes of the kernels. For example, if a convolutional layer with 32 kernels of nominal size $3\\times 3$ receives 16 feature maps on its input, it will require $16\\times 3\\times 3\\times 32+32$ where the last 32 parameters refer to the kernel biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms42xQeSVkOO"
   },
   "source": [
    "### Google Colab preliminaries\n",
    "\n",
    "Upload the zipped cnn_img folder to Google Colab and ensure the paths in the notebook are adjusted accordingly.\n",
    "\n",
    "If the notebook is not run on Google Colab, skip the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kmh4zv-2rzmX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8Wz1GQ5ZUlZ"
   },
   "outputs": [],
   "source": [
    "!unzip /content/cnn_img.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJdpRk2gV9q4"
   },
   "source": [
    "## 8.1 The MNIST dataset revisited (2)\n",
    "In one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a convolutional neural network to the digit classification problem. We will use the following layers to build our model:\n",
    "\n",
    "* [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
    "* [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    "* [torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n",
    "* [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "\n",
    "The [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer has the same effect as the fully connected layer, a matrix multiplication that was used in the previous exercise with the MNIST dataset.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. Study and run the code below. How is the accuracy compared to the ones obtained in the previous exerises with MNIST?\n",
    "\n",
    "    The accuracy is greater than in previous exercises.\n",
    "\n",
    "\n",
    "2. Try to change the number and size of convolutional and fully connected layers. What has the greatest impact on the accuracy? For each network architecture configuration calculate the number of trainable parameters.\n",
    "\n",
    "    Number of channels.\n",
    "\n",
    "3. What happens to the accuracy if another [non-linearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) is used instead of ReLU? Experiment with at least two different activation functions.\n",
    "\n",
    "    I tried LeakyRELU and Tanh function. Accuracies are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m_1TWwNif8WE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NK79DP85edIb"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "  # Method that defines the layers and other components of a model\n",
    "  def __init__(self,\n",
    "               input_channels,\n",
    "               n_channels_1,\n",
    "               n_channels_2,\n",
    "               n_fully_connected,\n",
    "               n_classes,\n",
    "               kernel_size\n",
    "               ):\n",
    "\n",
    "    super(CNN, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
    "                           out_channels=n_channels_1,\n",
    "                           kernel_size=kernel_size,\n",
    "                           padding='same'\n",
    "                           )\n",
    "\n",
    "    self.relu1 = nn.ReLU()\n",
    "\n",
    "    self.maxpool1 = nn.MaxPool2d((2,2))\n",
    "\n",
    "    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n",
    "                           out_channels=n_channels_2,\n",
    "                           kernel_size=kernel_size,\n",
    "                           padding='same'\n",
    "                           )\n",
    "\n",
    "    self.relu2 = nn.ReLU()\n",
    "\n",
    "    self.maxpool2 = nn.MaxPool2d((2,2))\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected, bias=True)\n",
    "\n",
    "    self.relu3 = nn.ReLU()\n",
    "\n",
    "    self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n",
    "\n",
    "  # Method where the computation gets done\n",
    "  def forward(self, x):\n",
    "\n",
    "    # First convolutional layer\n",
    "    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n",
    "    # We are padding the input in order for the result to have the same number of rows and columns\n",
    "    x = self.conv1(x)\n",
    "\n",
    "    # Applying the non-linearity\n",
    "    x = self.relu1(x)\n",
    "\n",
    "    # and max pooling again, now each feature map will be of size 7 X 7\n",
    "    x = self.maxpool1(x)\n",
    "\n",
    "    # Second convolutional layer\n",
    "    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n",
    "    x = self.conv2(x)\n",
    "\n",
    "    # again, we apply the non-linearity\n",
    "    x = self.relu2(x)\n",
    "\n",
    "    # and max pooling again, now each feature map will be of size 7 X 7\n",
    "    x = self.maxpool2(x)\n",
    "\n",
    "    # Flatten all dimensions except the batch\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    # Fully connected layer\n",
    "    x = self.fc1(x)\n",
    "\n",
    "    # and again, we apply the non-linearity\n",
    "    x = self.relu3(x)\n",
    "\n",
    "    # Non-linearity\n",
    "    pred_logits = self.fc2(x)\n",
    "\n",
    "    return pred_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FFJtOH6oNfLR"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_dataloader, optimizer, epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm.tqdm(train_dataloader):\n",
    "\n",
    "      # Every data instance is an input image + label pair\n",
    "      images, labels = batch\n",
    "\n",
    "      # It is necessary to have both the model, and the data on the same device, either CPU or GPU, for the model to process data.\n",
    "      # Data on CPU and model on GPU, or vice-versa, will result in a Runtime error.\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "      # Zero your gradients for every batch\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Make predictions for this batch\n",
    "      pred_logits = model(images)\n",
    "\n",
    "      # Compute the loss\n",
    "      loss = loss_fn(pred_logits, labels)\n",
    "\n",
    "      # Calculates the backward gradients over the learning weights\n",
    "      loss.backward()\n",
    "\n",
    "      # Tells the optimizer to perform one learning step\n",
    "      # Adjust the model�s learning weights based on the observed gradients for this batch\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    # Print epoch's average loss\n",
    "    print(\"Epoch {} - Training loss: {}\".format(epoch+1, train_loss/len(train_dataloader)))\n",
    "\n",
    "\n",
    "def evaluation(model, device, test_dataloader, epoch):\n",
    "\n",
    "    # Sets layers like dropout and batch normalization to evaluation mode before running inference\n",
    "    # Failing to do this will yield inconsistent inference results\n",
    "    model.eval()\n",
    "\n",
    "    test_accuracy = 0.0\n",
    "\n",
    "    # Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward().\n",
    "    # It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
    "    with torch.no_grad():\n",
    "\n",
    "      for batch in tqdm.tqdm(test_dataloader):\n",
    "\n",
    "        images, labels = batch\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        pred_logits = model(images)\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(pred_logits, dim=1)\n",
    "\n",
    "        # Find the index of the highest probability\n",
    "        predictions = probabilities.argmax(dim=1)\n",
    "\n",
    "        # Caluculate average batch accuracy\n",
    "        batch_accuracy = torch.mean((predictions == labels).float())\n",
    "\n",
    "        test_accuracy += batch_accuracy\n",
    "\n",
    "      print(\"Epoch {} - Accuracy: {}\".format(epoch+1, test_accuracy/len(test_dataloader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xpUS9zIEoZ4p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Architecture configs\n",
    "input_channels=1\n",
    "n_channels_1=32\n",
    "n_channels_2=64\n",
    "n_classes=10\n",
    "n_fully_connected=128\n",
    "kernel_size=5\n",
    "\n",
    "# Training configs\n",
    "training_epochs_count = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "display_step=1\n",
    "\n",
    "# Model\n",
    "model = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n",
    "\n",
    "# Move model to GPU if possible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Augmentations\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Stores the samples and their corresponding labels\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "# Wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dPXNNzNl7Lzb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:01<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.11848155183788103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:05<00:00, 31.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Accuracy: 0.987659215927124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:03<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.038646753586611265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 36.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Accuracy: 0.9914410710334778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:03<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 0.02694044200202593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Accuracy: 0.9923368096351624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:02<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 0.01887487377249167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 36.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Accuracy: 0.9915406107902527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:02<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 0.01541042047863924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 36.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Accuracy: 0.990545392036438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(training_epochs_count):\n",
    "\n",
    "  train_epoch(model, device, train_dataloader, optimizer, epoch)\n",
    "\n",
    "  if (epoch + 1) % display_step == 0:\n",
    "\n",
    "    evaluation(model, device, test_dataloader, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:03<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.13187366523204058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 36.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Accuracy: 0.984574019908905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:03<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.04206267884150068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 154/157 [00:04<00:00, 34.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m train_epoch(model, device, train_dataloader, optimizer, epoch)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m display_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m   \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model, device, test_dataloader, epoch)\u001b[0m\n\u001b[1;32m     52\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     54\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 56\u001b[0m pred_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(pred_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Find the index of the highest probability\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Faks/NEUMRE/Labosi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Faks/NEUMRE/Labosi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool1(x)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Second convolutional layer\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# We will apply n_channels_2 kernels of size kernel_size X kernel_size\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# again, we apply the non-linearity\u001b[39;00m\n\u001b[1;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(x)\n",
      "File \u001b[0;32m~/Desktop/Faks/NEUMRE/Labosi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Faks/NEUMRE/Labosi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Faks/NEUMRE/Labosi/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Faks/NEUMRE/Labosi/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TASK 2, lowered the number of fully connected neurons\n",
    "\n",
    "# Architecture configs\n",
    "input_channels=1\n",
    "n_channels_1=32\n",
    "n_channels_2=64\n",
    "n_classes=10\n",
    "n_fully_connected=64\n",
    "kernel_size=5\n",
    "\n",
    "# Training configs\n",
    "training_epochs_count = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "display_step=1\n",
    "\n",
    "# Model\n",
    "model = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n",
    "\n",
    "# Move model to GPU if possible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Augmentations\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Stores the samples and their corresponding labels\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "# Wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(training_epochs_count):\n",
    "\n",
    "  train_epoch(model, device, train_dataloader, optimizer, epoch)\n",
    "\n",
    "  if (epoch + 1) % display_step == 0:\n",
    "\n",
    "    evaluation(model, device, test_dataloader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:36<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.1517795866868123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 64.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Accuracy: 0.9860668778419495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TASK 2, lowered the number of channels\n",
    "\n",
    "# Architecture configs\n",
    "input_channels=1\n",
    "n_channels_1=16\n",
    "n_channels_2=32\n",
    "n_classes=10\n",
    "n_fully_connected=128\n",
    "kernel_size=5\n",
    "\n",
    "# Training configs\n",
    "training_epochs_count = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "display_step=1\n",
    "\n",
    "# Model\n",
    "model = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n",
    "\n",
    "# Move model to GPU if possible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Augmentations\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Stores the samples and their corresponding labels\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "# Wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(1):\n",
    "\n",
    "  train_epoch(model, device, train_dataloader, optimizer, epoch)\n",
    "\n",
    "  if (epoch + 1) % display_step == 0:\n",
    "\n",
    "    evaluation(model, device, test_dataloader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:06<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.14574460173224305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Accuracy: 0.9862658977508545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TASK 3, Tanh as activation function\n",
    "\n",
    "# Architecture configs\n",
    "input_channels=1\n",
    "n_channels_1=32\n",
    "n_channels_2=64\n",
    "n_classes=10\n",
    "n_fully_connected=64\n",
    "kernel_size=5\n",
    "\n",
    "# Training configs\n",
    "training_epochs_count = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "display_step=1\n",
    "\n",
    "# Model\n",
    "model = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n",
    "model.relu1 = nn.Tanh()\n",
    "model.relu2 = nn.Tanh()\n",
    "model.relu3 = nn.Tanh()\n",
    "\n",
    "# Move model to GPU if possible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Augmentations\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Stores the samples and their corresponding labels\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "# Wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(1):\n",
    "\n",
    "  train_epoch(model, device, train_dataloader, optimizer, epoch)\n",
    "\n",
    "  if (epoch + 1) % display_step == 0:\n",
    "\n",
    "    evaluation(model, device, test_dataloader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:05<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.12939571762675428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 36.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Accuracy: 0.9791003465652466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TASK 3, LeakyRELU as activation function\n",
    "\n",
    "# Architecture configs\n",
    "input_channels=1\n",
    "n_channels_1=32\n",
    "n_channels_2=64\n",
    "n_classes=10\n",
    "n_fully_connected=64\n",
    "kernel_size=5\n",
    "\n",
    "# Training configs\n",
    "training_epochs_count = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "display_step=1\n",
    "\n",
    "# Model\n",
    "model = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n",
    "model.relu1 = nn.LeakyReLU()\n",
    "model.relu2 = nn.LeakyReLU()\n",
    "model.relu3 = nn.LeakyReLU()\n",
    "\n",
    "# Move model to GPU if possible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Augmentations\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Stores the samples and their corresponding labels\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "# Wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(1):\n",
    "\n",
    "  train_epoch(model, device, train_dataloader, optimizer, epoch)\n",
    "\n",
    "  if (epoch + 1) % display_step == 0:\n",
    "\n",
    "    evaluation(model, device, test_dataloader, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsU9MOc8QmDe"
   },
   "source": [
    "## 8.2 Image classification\n",
    "Image classification is a challenging computer vision problem with the best-known competition being [The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/), which includes the ImageNet dataset with millions of $224\\times 224$ training images. The class names in one of the tasks there can be found [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). One of the most important breakthroughs was when in 2012 the convolutional neural network [AlexNet](https://en.wikipedia.org/wiki/AlexNet) won the first place. Ever since many highly successful convolutional neural networks architectures have been proposed, e.g. [VGG-16](https://arxiv.org/abs/1409.1556), [VGG-19](https://arxiv.org/abs/1409.1556), [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), [Inception](https://arxiv.org/abs/1409.4842), etc. Training such networks requires a lot of time because they have many layers with millions of parameters. In this exercise we are going to experiment with pre-trained models of some of the best known architectures.\n",
    "\n",
    "### 8.2.1 Using pre-trained models\n",
    "Try running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOJamRVpQrdo"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.io import read_image\n",
    "\n",
    "### Choose the architecture\n",
    "architecture=\"resnet34\"\n",
    "#architecture=\"vgg16\"\n",
    "#architecture=\"vgg19\"\n",
    "#architecture=\"inceptionv3\"\n",
    "\n",
    "if architecture == \"resnet34\":\n",
    "  weights = models.ResNet34_Weights.DEFAULT\n",
    "  model = models.resnet34(weights=weights)\n",
    "elif architecture == \"vgg16\":\n",
    "  weights = models.VGG16_Weights.DEFAULT\n",
    "  model = models.vgg16(pretrained=weights)\n",
    "elif architecture == \"vgg19\":\n",
    "  weights = models.VGG19_Weights.DEFAULT\n",
    "  model = models.vgg19(pretrained=weights)\n",
    "elif architecture == \"inceptionv3\":\n",
    "  weights = models.Inception_V3_Weights.DEFAULT\n",
    "  model = models.inception_v3(pretrained=weights)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "image_paths=[\"/content/cnn_img/badger.jpg\", \"/content/cnn_img/rabbit.jpg\", \"/content/cnn_img/sundial.jpg\", \"/content/cnn_img/pineapple.jpg\", \"/content/cnn_img/can.jpg\"]\n",
    "\n",
    "for path in image_paths:\n",
    "    #loading the image and rescaling it to fit the size for the imagenet architectures\n",
    "    img = read_image(path)\n",
    "    preprocess = weights.transforms(antialias=True)\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "    prediction = model(batch).squeeze(0).softmax(0)\n",
    "    class_id = prediction.argmax().item()\n",
    "    score = prediction[class_id].item()\n",
    "    category_name = weights.meta[\"categories\"][class_id]\n",
    "\n",
    "    print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZmSyBglaUoa"
   },
   "source": [
    "**Tasks**\n",
    "1. Is there any significant difference between the results of different architectures?\n",
    "2. Try to classify several other images from the folders cnn_img/healthy and cnn_img/unhealthy that you choose on your own. Which cases are problematic?\n",
    "\n",
    "### 8.2.2 Creating your own classifier - pincers vs. scissors\n",
    "Although ImageNet has a lot of classes, sometimes they do not cover some desired cases. Let's assume that we want to tell images with pincers apart from the ones with scissors. Neither pincers nor scissors are among ImageNet classes. Nevertheless, we can still use some parts of the pre-trained models.\n",
    "\n",
    "Various layers of a deep convolutional network have diferent tasks. The ones closest to the original input image usually look for features such as edges and corners i.e. for low-level features. After them there are layers that look for middle-level features such as circular objects, special curves, etc. Next, there are usually fully connected layers that create high-level semantic features by combining the information from the previous layers. These features are then used by the last layer that performs the actual classification. What we can do here is simply to discard the last layer i.e. not to calculate the class of an image, but to extract the values in on of the fully connected layers. This effectively means that we are going to use the network only as an extractor for high-level features that we would hardly be able to engineer on our own. Let's first see which layers can be found in the ResNet network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztcpyruTRTJD"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "architecture=\"resnet34\"\n",
    "\n",
    "if architecture == \"resnet34\":\n",
    "  weights = models.ResNet34_Weights.DEFAULT\n",
    "  base_model = models.resnet34(weights=weights)\n",
    "elif architecture == \"resnet50\":\n",
    "  weights = models.ResNet50_Weights.DEFAULT\n",
    "  base_model = models.resnet50(weights=weights)\n",
    "elif architecture == \"vgg16\":\n",
    "  weights = models.VGG16_Weights.DEFAULT\n",
    "  base_model = models.vgg16(weights=weights)\n",
    "\n",
    "for layer in base_model.children():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acvV9u06GocO"
   },
   "source": [
    "At the end you can see fully connected layer used for classification. We can extract the values from previous layers by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gxut2qMPJS3Y"
   },
   "outputs": [],
   "source": [
    "# Model without last fully connected layer\n",
    "model = torch.nn.Sequential(*(list(base_model.children())[:-1]), nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "img_path=\"/content/cnn_img/rabbit.jpg\"\n",
    "\n",
    "img = read_image(img_path)\n",
    "preprocess = weights.transforms(antialias=True)\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "features = model(batch).squeeze(3).squeeze(2)\n",
    "\n",
    "print(features.shape)\n",
    "feature_layer_size=features.shape[1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjTMbaFPKfvk"
   },
   "source": [
    "These values can now be used as features and that can later be used with another classifier. Let's first extract the features for our pincer and scissors images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPNycPJ4KhBQ"
   },
   "outputs": [],
   "source": [
    "def create_numbered_paths(home_dir, n):\n",
    "    return [home_dir+str(i)+\".jpg\" for i in range(n)]\n",
    "\n",
    "def create_paired_numbered_paths(first_home_dir, second_home_dir, n):\n",
    "    image_paths=[]\n",
    "    for p in zip(create_numbered_paths(first_home_dir, n), create_numbered_paths(second_home_dir, n)):\n",
    "        image_paths.extend(p)\n",
    "    return image_paths\n",
    "\n",
    "def create_features(paths, verbose=True):\n",
    "    n=len(paths)\n",
    "    features=np.zeros((n, feature_layer_size))\n",
    "    for i in range(n):\n",
    "        if (verbose==True):\n",
    "            print(\"\\t%2d / %2d\"%(i+1, n))\n",
    "        img = read_image(paths[i])\n",
    "        preprocess = weights.transforms(antialias=True)\n",
    "        batch = preprocess(img).unsqueeze(0)\n",
    "        features[i, :]=model(batch).squeeze(3).squeeze(2).detach().numpy()\n",
    "\n",
    "    return features\n",
    "\n",
    "pincers_dir=\"/content/cnn_img/pincers/\"\n",
    "scissors_dir=\"/content/cnn_img/scissors/\"\n",
    "\n",
    "individual_n=50\n",
    "\n",
    "#combining all image paths\n",
    "image_paths=create_paired_numbered_paths(pincers_dir, scissors_dir, individual_n)\n",
    "\n",
    "#marking their classes\n",
    "image_classes=[]\n",
    "for i in range(individual_n):\n",
    "    #0 stands for the pincer image and 0 stands for the scissors image\n",
    "    image_classes.extend((0, 1))\n",
    "\n",
    "#number of all images\n",
    "n=100\n",
    "#number of training images\n",
    "n_train=50\n",
    "#number of test images\n",
    "n_test=n-n_train\n",
    "\n",
    "print(\"Creating training features...\")\n",
    "#here we will store the features of training images\n",
    "x_train=create_features(image_paths[:n_train])\n",
    "#train classes\n",
    "y_train=np.array(image_classes[:n_train])\n",
    "\n",
    "print(\"Creating test features...\")\n",
    "#here we will store the features of test images\n",
    "x_test=create_features(image_paths[n_train:])\n",
    "\n",
    "#train classes\n",
    "y_test=np.array(image_classes[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQQp23pwweEf"
   },
   "source": [
    "Now that for each image we have its features, we will divide the images into a training and a test set. Then we will use a linear SVM classifier to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R12r9KSrgt95"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def create_svm_classifier(x, y, C=1.0, kernel='linear'):\n",
    "    #we will use linear SVM\n",
    "    classifier=svm.SVC(kernel=kernel, C=C);\n",
    "    classifier.fit(x, y)\n",
    "    return classifier\n",
    "\n",
    "def calculate_accuracy(classifier, x, y):\n",
    "    predicted=classifier.predict(x)\n",
    "    return np.sum(y==predicted)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADOM91TWwdmS"
   },
   "outputs": [],
   "source": [
    "#training the model\n",
    "classifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\n",
    "\n",
    "#checking the model's accuracy\n",
    "print(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSMhoT6swkeZ"
   },
   "source": [
    "**Tasks**\n",
    "\n",
    "1. Is there any significant gain if more complex SVM models are used?\n",
    "2. What happens if we extract features using different backbone, e.g. vgg16?\n",
    "\n",
    "\n",
    "### 8.2.1 Creating your own classifier - healthy vs. unhealthy food\n",
    "The previous example was relatively simple because all images were of same size and each of them had a white background, which allowed the extractor to concentrate only on the features of the actual objects. In this example we will use a slightly more complicated case - namely, will will tell images with healthy food apart from the ones with unhealthy food. FIrst let's repeat the same process as we did in the previous example and create the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JI75TDrVxNrR"
   },
   "outputs": [],
   "source": [
    "healthy_dir=\"/content/cnn_img/healthy/\"\n",
    "unhealthy_dir=\"/content/cnn_img/unhealthy/\"\n",
    "\n",
    "individual_n=100\n",
    "\n",
    "#combining all image paths\n",
    "image_paths=create_paired_numbered_paths(healthy_dir, unhealthy_dir, individual_n)\n",
    "\n",
    "#marking their classes\n",
    "image_classes=[]\n",
    "for i in range(individual_n):\n",
    "    #0 stands for the pincer image and 0 stands for the scissors image\n",
    "    image_classes.extend((0, 1))\n",
    "\n",
    "#number of all images\n",
    "n=200\n",
    "#number of training images\n",
    "n_train=100\n",
    "#number of test images\n",
    "n_test=n-n_train\n",
    "\n",
    "print(\"Creating training features...\")\n",
    "#here we will store the features of training images\n",
    "x_train=create_features(image_paths[:n_train])\n",
    "#train classes\n",
    "y_train=np.array(image_classes[:n_train])\n",
    "\n",
    "print(\"Creating test features...\")\n",
    "#here we will store the features of test images\n",
    "x_test=create_features(image_paths[n_train:])\n",
    "#train classes\n",
    "y_test=np.array(image_classes[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfezje2qxQns"
   },
   "source": [
    "Now let's train a model and test its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXUOBrkjxNzU"
   },
   "outputs": [],
   "source": [
    "classifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\n",
    "print(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Zt5OzoxWqj"
   },
   "source": [
    "**Tasks**\n",
    "1. Try the whole food classification with another network as a feature extractor and compare their results.\n",
    "2. What kind of test images are problematic?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
